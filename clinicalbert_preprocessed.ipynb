{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install\n",
    "Before we begin, if you don't already have it you will need to install the following packages. Here is the install command:\n",
    "\n",
    "**transformers**: `conda install -c conda-forge transformers`\n",
    "\n",
    "It's important to note that my code differs from Kexin's because I [migrated](https://huggingface.co/transformers/migration.html) to using [HuggingFace's](https://huggingface.co/transformers/index.html) new `transformer` module instead of the formerly known as `pytorch_pretrained_bert` that the author used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read this article for ClinicalBERT\n",
    "https://arxiv.org/pdf/1904.05342.pdf\n",
    "They develop ClinicalBert by applying BERT (bidirectional encoder representations from transformers) to clinical notes. \n",
    "\n",
    "```\n",
    "@article{clinicalbert,\n",
    "author = {Kexin Huang and Jaan Altosaar and Rajesh Ranganath},\n",
    "title = {ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission},\n",
    "year = {2019},\n",
    "journal = {arXiv:1904.05342},\n",
    "}\n",
    "```\n",
    "\n",
    "# How My Work Differs from the Author's\n",
    "1. I am not pre-training the ClinicalBERT because the author already performed pre-training on Clinical words and the model's weights are already available.\n",
    "2. I am only working with early clinical notes. \"Discharge summaries have predictive power for readmission. However, discharge summaries might be written after a patient has left the hospital. Therefore, discharge summaries are not actionable since doctors cannot intervene when a patient has left the hospital. Models that dynamically predict readmission in the early stages of a patient's admission are relevant to clinicians...a maximum of the first 48 or 72 hours of a patient's notes are concatenated. These concatenated notes are used to predict readmission.\"[pg 12](https://arxiv.org/pdf/1904.05342.pdf)\n",
    "\n",
    "\n",
    "<img src=\"./images/fig1.png\" width=\"800\" />\n",
    "\n",
    "In this example, care providers add notes to an electronic health record during a patient’s admission, and the model dynamically updates the patient’s risk of being readmitted within a 30-day window.\n",
    "\n",
    "\n",
    "Boag et al. (2018) study the performance of the bag-of-words model, word2vec, and a Long Short-Term Memory Network (lstm) model combined with word2vec on various tasks such as diagnosis prediction and mortality risk estimation. Word embedding models such as word2vec are trained using the local context of individual words, but as clinical notes are long and their words are interdependent (Zhang et al., 2018), these methods cannot capture long-range dependencies.\n",
    "\n",
    "Clinical notes require capturing interactions between distant words.\n",
    "\n",
    "In this work, they develop a model that can predict readmission dynamically. **Making a prediction using a discharge summary at the end of a stay means that there are fewer opportunities to reduce the chance of readmission. To build a clinically-relevant model, we define a task for predicting readmission at any timepoint since a patient was admitted.**\n",
    "\n",
    "Medicine suffers from alarm fatigue (Sendelbach and Funk, 2013). This\n",
    "means useful classification rules for medicine need to have high precision (positive predictive value).\n",
    "\n",
    "Compared to a popular model of clinical text, word2vec, ClinicalBert more accurately captures clinical word similarity.\n",
    "\n",
    "ClinicalBERT is a modified BERT model: Specifically, the representations are learned\n",
    "using medical notes and further processed for downstream clinical tasks.\n",
    "* The transformer encoder architecture is based on a self-attention mechanism\n",
    "* The pre-training objective function for the model is defined using two unsupervised tasks: masked language modeling and next sentence prediction. \n",
    "* The text embeddings and model parameters are fit using stochastic optimization.\n",
    "\n",
    "<img src=\"./images/fig2.png\" width=\"800\" />\n",
    "\n",
    "ClinicalBert learns deep representations of clinical text using two unsupervised language modeling tasks: masked language modeling and\n",
    "next sentence prediction\n",
    "\n",
    "### Clinical Text Embeddings\n",
    "A clinical note input to ClinicalBert is represented as a collection of tokens. In ClinicalBert, a token in a clinical note is computed as\n",
    "the sum of the token embedding, a learned segment embedding, and a position embedding.\n",
    "\n",
    "### Pre-training ClinicalBERT\n",
    "The quality of learned representations of text depends on the text the model was trained on. BERT is trained on BooksCorpus and Wikipedia. However, these two datasets are distinct from clinical notes (where jargon and abbreviations are common). Also clinical notes have different syntax and grammar than common language in books or encyclopedias. It is hard to understand clinical notes without professional training.\n",
    "\n",
    "ClinicalBERT improves over BERT on the MIMIC-III corpus of clinical notes for \n",
    "1. Accuracy of masked language modeling a.k.a. predicting held-out tokens (86.80% vs 56.80%).\n",
    "2. Next sentence prediction (99.25% vs. 80.50%).\n",
    "The pre-training objective function based on the two tasks is the sum of the log-likelihood of the masked tokens and the log-likelihood of the binary variable indicating whether two sentences are consecutive.\n",
    "\n",
    "### Fine-tuning ClinicalBERT\n",
    "The model parameters are fine-tuned to maximize the log-likelihood of this binary classifier: equation (2)\n",
    "\n",
    "##  Empirical Study II: 30-Day Hospital Readmission Prediction\n",
    "Before the author even evaluated ClinicalBERT's performance as a model of readmission, **his initial experiment showed that the original BERT suffered in performance on the masked language modeling task on the MIMIC-III data as well as the next sentence prediction tasks. This proves the need develop models tailored to clinical data such as ClinicalBERT!**\n",
    "\n",
    "<img src=\"./images/equ3.png\" width=\"600\" />\n",
    "\n",
    "He finds that computing readmission probability using Equation (3) consistently outperforms predictions on each subsequence individually by 3–8%. This is because\n",
    "1. some subsequences (such as tokens corresponding to progress reports) do NOT contain information about readmission, whereas others do. The risk of readmission should be computed using subsequences that correlate with readmission risk, and **the effect of unimportant subsequences should be minimized**. This is accomplished by using the maximum probability over subsequences. \n",
    "2. Also noisy subsequences mislead the model and decrease performance. So they also include the average probability of readmission across subsequences. This leads to a trade-off between the mean and maximum probabilities of readmission in Equation (3).\n",
    "3. if there are a large number of subsequences for a patient with many clinical notes, there is a higher probability of having a noisy maximum probability of readmission. This means longer sequences may need to have a larger weight on the mean prediction. We include this weight as the n/c scaling factor, with c adjusting for patients with many clinical notes.\n",
    "Empirically, he found that c = 2 performs best on validation data.\n",
    "\n",
    "### Evaluation\n",
    "For validation and testing, 10% of the data is held out respectively, and 5-fold cross-validation is conducted. \n",
    "\n",
    "Each model is evaluated using three metrics:\n",
    "1. AUROC\n",
    "2. Area under the precision-recall curve\n",
    "3. Recall at precision of 80%: For the readmission task, false positives are important. To minimize the number of false positives and thus minimize the risk of alarm fatigue, he set the precision to 80% (in other words, 20% false positives out of the predicted positive class) and use the corresponding threshold to calculate recall. This leads to a clinically-relevant metric that enables us to build models that control the false positive rate. \n",
    "\n",
    "### Models\n",
    "* The training parameters are the entire encoder network, along with the classifier **`W`**\n",
    "* Note that the data labels are imbalanced: negative labels are subsampled to balance the positive readmit labels\n",
    "* ClinicalBert is trained for one epoch with batch size 4 and ee use the Adam optimizer learning rate 2 × 10−5\n",
    "*  The ClinicalBert model settings are the same as in Section 3.\n",
    "* The binary classifier is a linear layer of shape 768 × 1\n",
    "* The maximum sequence length supported by the model is set to 512, and the model is first trained using shorter sequences.\n",
    "\n",
    "<img src=\"./images/tab3.png\" width=\"600\" />\n",
    "\n",
    "Shows that ClinicalBERT outperforms it's competitors like Bag-of-words (Top 5000 TF-IDF words as features) and BiLSTM/Word2Vec in terms of precision and recall.\n",
    "\n",
    "###  Readmission Prediction With Early Clinical Notes\n",
    "Discharge summaries have predictive power for readmission. However, discharge summaries\n",
    "might be written after a patient has left the hospital. Therefore, discharge summaries are\n",
    "not actionable since doctors cannot intervene when a patient has left the hospital. Models\n",
    "that dynamically predict readmission in the early stages of a patient’s admission are relevant to clinicians.\n",
    "\n",
    "> **Note** that readmission predictions from a model are not actionable if a patient has been discharged. \n",
    "\n",
    "**24-48h**\n",
    "* In the MIMIC-III data, admission and discharge times are available, but clinical notes do not have timestamps. This is why the table headings show a range; this range shows the cutoff time for notes fed to the model from early on in a patient’s admission. For example, in the 24–48h column, the model may only take as input a patient’s notes up to 36h because of that patient’s specific admission time.\n",
    "\n",
    "**48-72h**\n",
    "* For the second set of readmission prediction experiments, a maximum of the first 48 or 72 hours of a patient’s notes are concatenated. These concatenated notes are used to predict readmission. Since we separate notes into subsequences of the same length, the training set consists of all subsequences within a maximum of 72 hours, and the model is tested given only available notes within the first 48 or 72 hours of a patient’s admission.\n",
    "* For testing 48 or 72-hour clinical note readmission prediction, patients that are discharged within 48 or 72 hours (respectively) are filtered out.\n",
    "\n",
    "### Interpretable predictions in ClinicalBert\n",
    "* ClinicalBert uses several self-attention mechanisms which can be used to inspect its predictions, by visualizing terms correlated with predictions of hospital readmission.\n",
    "    * For every clinical note input to ClinicalBert, each self-attention mechanism computes a distribution over every term in a sentence, given a query.\n",
    "    * **A high attention weight between a query and key token means the interaction between these tokens is predictive of readmission**.\n",
    "    *  In the ClinicalBert encoder, there are 144 self-attention mechanisms (or, 12 multi-head attention mechanisms for each of the 12 transformer encoders). \n",
    "  \n",
    "\n",
    "### Preprocessing\n",
    "ClinicalBert requires minimal preprocessing:\n",
    "1. First, words are converted to lowercase and\n",
    "2. line breaks are removed\n",
    "3. carriage returns are removed. \n",
    "4. De-identified the brackets \n",
    "5. remove special characters like ==, −−\n",
    "\n",
    "* The SpaCy sentence segmentation package is used to segment each note (Honnibal and Montani, 2017).\n",
    "    * Since clinical notes don't follow rigid standard language grammar, we find rule-based segmentation has better results than dependency parsing-based segmentation.\n",
    "    * Various segmentation signs that misguide rule-based segmentators are removed or replaced\n",
    "        * For example 1.2 would be removed\n",
    "        * M.D., dr. would be replaced with with MD, Dr\n",
    "    * Clinical notes can include various lab results and medications that also contain numerous rule-based separators, such as 20mg, p.o., q.d.. (where q.d. means one a day and q.o. means to take by mouth.  \n",
    "        *  To address this, segmentations that have less than 20 words are fused into the previous segmentation so that they are not singled out as different sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 21:21:21 - INFO - numexpr.utils -   Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "12/14/2022 21:21:21 - INFO - numexpr.utils -   NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionable_ID_2days = df_adm[df_adm['DURATION'] >= 2].HADM_ID\n",
    "test_actionable_id_label_2days = test_id_label[test_id_label.id.isin(actionable_ID_2days)]\n",
    "early_test_2days = df_less_2[df_less_2.ID.isin(test_actionable_id_label_2days.id)]\n",
    "early_test_2days.to_csv('./data/2days/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model for Predicting Readmission Using Early Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.9/site-packages (1.13.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.9/site-packages (from torch) (3.10.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device-Agnostic PyTorch code (GPU or CPU)\n",
    "A `torch.device` is an object representing the device on which a `torch.Tensor` is or will be allocated. [[Docs](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device)]. The `torch.device` contains a device type (`'cpu'` or `'cuda'`).\n",
    "Due to the structure of PyTorch, you may need to explicitly write device-agnostic (CPU or GPU) code [[Docs](https://pytorch.org/docs/stable/notes/cuda.html)]. The first step is to determine whether the GPU should be used or not.\n",
    "\n",
    "`torch.cuda.is_available` returns a bool indicating if CUDA is currently available [[Docs](https://pytorch.org/docs/stable/cuda.html#torch.cuda.is_available)]. \n",
    "\n",
    "I will set my values below so that my code is **Device-agnostic** but feel free to change them for your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rank = -1\n",
    "no_cuda = False # Set flag to True to disable CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank == -1 or no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 21:22:38 - INFO - __main__ -   device: cpu n_gpu: 0 Distributed training: False\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"device: %s n_gpu: %d Distributed training: %r\", device, n_gpu, bool(local_rank != -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accumulating gradients** just means that, before calling `optimizer.step()` to perform a step of gradient descent, we will sum the gradients of several backward operations in the `parameter.grad` tensors. \n",
    "\n",
    "Below I set the number of update steps to accumulate before performing a backward/update pass. I will set it to a default of 1. Feel free to change if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gradient_accumulation_steps < 1:\n",
    "    raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(gradient_accumulation_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default Total Batch Size for training to 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = int(train_batch_size / gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed= 42 # random seed for initialization\n",
    "do_train = False # Whether to run training\n",
    "do_eval = True # Whether to run eval on the dev set.\n",
    "output_dir = './result_early' # The output directory where the model checkpoints will be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `result_early` folder where results will go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "if not do_train and not do_eval:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "if os.path.exists(output_dir) and os.listdir(output_dir):\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(output_dir))\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Classes Needed for Processing Readmissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "        \n",
    "    @classmethod\n",
    "    def _read_csv(cls, input_file):\n",
    "        \"\"\"Reads a comma separated value file.\"\"\"\n",
    "        file=pd.read_csv(input_file)\n",
    "        lines=zip(file.ID,file.TEXT,file.Label)\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class readmissionProcessor(DataProcessor):\n",
    "    def get_train_examples(self, data_dir):\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.csv\")))\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n",
    "    \n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"val.csv\")), \"val\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\"]\n",
    "    \n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[1]\n",
    "            label = str(int(line[2])) \n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = readmissionProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = processor.get_labels() # label_list = ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.9/site-packages (2.1.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.1.97)\n",
      "Requirement already satisfied: sacremoses in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.28)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from boto3->transformers) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in /opt/anaconda3/lib/python3.9/site-packages (from boto3->transformers) (1.27.59)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.9/site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/anaconda3/lib/python3.9/site-packages (from botocore<1.28.0,>=1.27.28->boto3->transformers) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.9/site-packages (from botocore<1.28.0,>=1.27.28->boto3->transformers) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.28->boto3->transformers) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 21:23:10 - INFO - transformers.file_utils -   PyTorch version 1.13.0 available.\n",
      "12/14/2022 21:23:10 - INFO - transformers.modeling_xlnet -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "During tokenization, each word in the sentence is broken apart into smaller and smaller tokens (word pieces) until all the tokens in the dataset are recognized by the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 21:25:20 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/lxu/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the input data directory. Should contain the .tsv files (or other data files) for the readmission task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/discharge/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is only needed if you want to pre-train ClinicalBERT. If you want to perform pre-training of BERT yourself, you should have set the variable `do_train` earlier to `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = None\n",
    "num_train_steps = None\n",
    "if do_train:\n",
    "    train_examples = processor.get_train_examples(data_dir)\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model\n",
    "To import a custom module into Jupyter notebook, use sys.path.append because Jupyter doesn't always see/find the module you uploaded. Thanks to this Stack Overflow [answer](https://stackoverflow.com/questions/53049195/importing-custom-module-into-jupyter-notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 23:14:37 - INFO - modeling_readmission -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/lxu/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "12/14/2022 23:14:37 - INFO - modeling_readmission -   extracting archive file /Users/lxu/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/lh/4k81x5k96n1_38nzplx6lg7r0000gp/T/tmp6_levzun\n",
      "12/14/2022 23:14:40 - INFO - modeling_readmission -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/14/2022 23:14:43 - INFO - modeling_readmission -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "12/14/2022 23:14:43 - INFO - modeling_readmission -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "from modeling_readmission import BertForSequenceClassification\n",
    "# from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bert_model` is the Bert pre-trained model selected from the list: \n",
    "* bert-base-uncased\n",
    "* bert-large-uncased\n",
    "* bert-base-cased\n",
    "* bert-base-multilingual\n",
    "* bert-base-chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main breaking change when migrating from pytorch-pretrained-bert to transformers is that the models forward method always outputs a tuple with various elements depending on the model and the configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model='./model/early_readmission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 21:23:28 - INFO - modeling_readmission -   loading archive file ./model/early_readmission\n",
      "12/14/2022 21:23:28 - INFO - modeling_readmission -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import modeling_readmission\n",
    "from modeling_readmission import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send data to the chosen device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`DistributedDataParallel`](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel) (DDP) implements data parallelism at the module level. It synchronizes gradients, parameters, and buffers. If your model is too large to fit on a single GPU, you must use model parallel to split it across multiple GPUs. DDP works with model parallel. DDP is multi-process and works for both single- and multi-machine training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank != -1:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Optimizer - AdamW (Weight Decay with Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *** Skip this section if you're using Pre-trained BERT and have set the flag `do_train`=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the BERT baseline model is typically done with AdamW, a variant of the Adam optimizer with weight decay as the optimizer. \n",
    "\n",
    ">At its heart, Adam is a simple and intuitive idea: why use the same learning rate for every parameter, when we know that some surely need to be moved further and faster than others? Since the square of recent gradients tells us how much signal we’re getting for each weight, we can just divide by that to ensure even the most sluggish weights get their chance to shine. Adam takes that idea, adds on the standard approach to momentum, and (with a little tweak to keep early batches from being biased) that’s it!...We should use **weight decay** with Adam, and not the L2 regularization that classic deep learning libraries implement [[fast.ai](https://www.fast.ai/2018/07/02/adam-weight-decay/#understanding-adamw-weight-decay-or-l2-regularization)].\n",
    "\n",
    "* `optimize_on_cpu` is whether to perform optimization and keep the optimizer averages on CPU.\n",
    "* `learning_rate` is the initial learning rate for Adam.\n",
    "* `warmup_proportion` is the proportion of training to perform linear learning rate warmup for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_on_cpu = False\n",
    "learning_rate = 5e-5\n",
    "warmup_proportion = 0.1\n",
    "\n",
    "# num_warmup_steps = warmup_proportion * float(num_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "train_loss=100000\n",
    "number_training_steps=1\n",
    "global_step_check=0\n",
    "train_loss_history=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the cells below to run AdamW optimizer if training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if optimize_on_cpu:\n",
    "#     param_optimizer = [(n, param.clone().detach().to('cpu').requires_grad_()) \\\n",
    "#                         for n, param in model.named_parameters()]\n",
    "# else:\n",
    "#     param_optimizer = list(model.named_parameters())\n",
    "# no_decay = ['bias', 'gamma', 'beta']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "#     ]\n",
    "\n",
    "# optimizer = AdamW(optimizer_grouped_parameters,\n",
    "#                      lr=learning_rate,\n",
    "#                      correct_bias=False)  # To reproduce old BertAdam specific behavior set correct_bias=False\n",
    "\n",
    "# # PyTorch scheduler\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "#                                             num_warmup_steps=num_warmup_steps,\n",
    "#                                             num_training_steps=num_training_steps) \n",
    "\n",
    "# if do_train:\n",
    "#     train_features = convert_examples_to_features(\n",
    "#         train_examples, label_list, max_seq_length, tokenizer)\n",
    "#     logger.info(\"***** Running training *****\")\n",
    "#     logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "#     logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "#     logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "#     all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "#     all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "#     all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "#     all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "#     train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "#     if local_rank == -1:\n",
    "#         train_sampler = RandomSampler(train_data)\n",
    "#     else:\n",
    "#         train_sampler = DistributedSampler(train_data)\n",
    "#     train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "#     model.train()\n",
    "#     for epo in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "#         tr_loss = 0\n",
    "#         nb_tr_examples, nb_tr_steps = 0, 0\n",
    "#         for step, batch in enumerate(train_dataloader):\n",
    "#             batch = tuple(t.to(device) for t in batch)\n",
    "#             input_ids, input_mask, segment_ids, label_ids = batch\n",
    "#             loss, logits = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "#             if n_gpu > 1:\n",
    "#                 loss = loss.mean() # mean() to average on multi-gpu.\n",
    "#             if gradient_accumulation_steps > 1:\n",
    "#                 loss = loss / gradient_accumulation_steps\n",
    "#             loss.backward()\n",
    "#             train_loss_history.append(loss.item())\n",
    "#             tr_loss += loss.item()\n",
    "#             nb_tr_examples += input_ids.size(0)\n",
    "#             nb_tr_steps += 1\n",
    "#             if (step + 1) % gradient_accumulation_steps == 0:\n",
    "#                 model.zero_grad()\n",
    "#                 global_step += 1\n",
    "\n",
    "#             if (step+1) % 200 == 0:\n",
    "#                 string = 'step '+str(step+1)\n",
    "#                 print (string)\n",
    "\n",
    "#         train_loss=tr_loss\n",
    "#         global_step_check=global_step\n",
    "#         number_training_steps=nb_tr_steps\n",
    "\n",
    "#     string = './pytorch_model_new_'+ readmission_mode +'.bin'\n",
    "#     torch.save(model.state_dict(), string)\n",
    "\n",
    "#     fig1 = plt.figure()\n",
    "#     plt.plot(train_loss_history)\n",
    "#     fig1.savefig('loss_history.png', dpi=fig1.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn # Base class for all neural network modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc, confusion_matrix, classification_report\n",
    "from funcsigs import signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "readmission_mode = 'early'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_score(df, score, readmission_mode, output_dir):\n",
    "    df['pred_score'] = score\n",
    "    df_sort = df.sort_values(by=['ID'])\n",
    "    #score \n",
    "    temp = (df_sort.groupby(['ID'])['pred_score'].agg(max)+df_sort.groupby(['ID'])['pred_score'].agg(sum)/2)/(1+df_sort.groupby(['ID'])['pred_score'].agg(len)/2)\n",
    "    x = df_sort.groupby(['ID'])['Label'].agg(np.min).values\n",
    "    df_out = pd.DataFrame({'logits': temp.values, 'ID': x})\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(x, temp.values)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label='Val (area = {:.3f})'.format(auc_score))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    string = 'auroc_clinicalbert_'+readmission_mode+'.png'\n",
    "    plt.savefig(os.path.join(output_dir, string))\n",
    "\n",
    "    return fpr, tpr, df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_curve_plot(y, y_score, readmission_mode, output_dir):\n",
    "    precision, recall, _ = precision_recall_curve(y, y_score)\n",
    "    area = auc(recall,precision)\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve: AUC={0:0.2f}'.format(area))\n",
    "    \n",
    "    string = 'auprc_clinicalbert_'+readmission_mode+'.png'\n",
    "\n",
    "    plt.savefig(os.path.join(output_dir, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_pr_curve(df, score, readmission_mode, output_dir):\n",
    "    df['pred_score'] = score\n",
    "    df_sort = df.sort_values(by=['ID'])\n",
    "    #score \n",
    "    temp = (df_sort.groupby(['ID'])['pred_score'].agg(max)+df_sort.groupby(['ID'])['pred_score'].agg(sum)/2)/(1+df_sort.groupby(['ID'])['pred_score'].agg(len)/2)\n",
    "    y = df_sort.groupby(['ID'])['Label'].agg(np.min).values\n",
    "    \n",
    "    precision, recall, thres = precision_recall_curve(y, temp)\n",
    "    pr_thres = pd.DataFrame(data =  list(zip(precision, recall, thres)), columns = ['prec','recall','thres'])\n",
    "    vote_df = pd.DataFrame(data =  list(zip(temp, y)), columns = ['score','label'])\n",
    "    \n",
    "    pr_curve_plot(y, temp, readmission_mode, output_dir)\n",
    "    \n",
    "    temp = pr_thres[pr_thres.prec > 0.799999].reset_index()\n",
    "    \n",
    "    rp80 = 0\n",
    "    if temp.size == 0:\n",
    "        print('Test Sample too small or RP80=0')\n",
    "    else:\n",
    "        rp80 = temp.iloc[0].recall\n",
    "        print('Recall at Precision of 80 is {}', rp80)\n",
    "\n",
    "    return rp80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        #print (example.label)\n",
    "        label_id = label_map[example.label]\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_seq_length` is the maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "\n",
    "`eval_batch_size` is the total batch size for eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128\n",
    "eval_batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 23:15:52 - INFO - __main__ -   *** Example ***\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   guid: test-0\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   tokens: [CLS] sin ##us ta ##chy ##card ##ia . generalized low voltage . delayed r wave progression with late pre ##cor ##dial q ##rs transition . findings are non - specific . clinical correlation is suggested . since the previous tracing of same date sin ##us ta ##chy ##card ##ia rate is faster . tracing # 1 sin ##us rhythm . delayed r wave progression with late pre ##cor ##dial q ##rs transition . generalized low q ##rs voltage . findings are non - specific . clinical correlation is suggested . since the previous tracing of the rate is faster and voltage is lower . title : 45 y / o man with h ##x of et ##oh ci ##rr ##hosis who arrived for planned liver transplant . [SEP]\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   input_ids: 101 8254 2271 11937 11714 11522 2401 1012 18960 2659 10004 1012 8394 1054 4400 14967 2007 2397 3653 27108 27184 1053 2869 6653 1012 9556 2024 2512 1011 3563 1012 6612 16902 2003 4081 1012 2144 1996 3025 16907 1997 2168 3058 8254 2271 11937 11714 11522 2401 3446 2003 5514 1012 16907 1001 1015 8254 2271 6348 1012 8394 1054 4400 14967 2007 2397 3653 27108 27184 1053 2869 6653 1012 18960 2659 1053 2869 10004 1012 9556 2024 2512 1011 3563 1012 6612 16902 2003 4081 1012 2144 1996 3025 16907 1997 1996 3446 2003 5514 1998 10004 2003 2896 1012 2516 1024 3429 1061 1013 1051 2158 2007 1044 2595 1997 3802 11631 25022 12171 25229 2040 3369 2005 3740 11290 22291 1012 102\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   label: 1 (id = 1)\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   *** Example ***\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   guid: test-1\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   tokens: [CLS] pa and lateral chest radio ##graphs . comparison : compared to radio ##graph from . findings : heart size is moderately enlarged , similar to prior study . there is mild pulmonary vascular redistribution . there is stable small left pl ##eur ##al e ##ff ##usion . interval removal of right pic ##c line . impression : card ##iom ##ega ##ly . no pneumonia . small left pl ##eur ##al e ##ff ##usion . 11 : 26 pm chest ( portable ap ) ; - 76 by same physician # reason : s / p ol ##t . eva ##l for collapse , over ##load admitting diagnosis : liver tx medical condition : 45 year old man des ##at ##uration ##s , int ##uba ##ted reason [SEP]\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   input_ids: 101 6643 1998 11457 3108 2557 27341 1012 7831 1024 4102 2000 2557 14413 2013 1012 9556 1024 2540 2946 2003 17844 11792 1010 2714 2000 3188 2817 1012 2045 2003 10256 21908 21449 25707 1012 2045 2003 6540 2235 2187 20228 11236 2389 1041 4246 14499 1012 13483 8208 1997 2157 27263 2278 2240 1012 8605 1024 4003 18994 29107 2135 1012 2053 18583 1012 2235 2187 20228 11236 2389 1041 4246 14499 1012 2340 1024 2656 7610 3108 1006 12109 9706 1007 1025 1011 6146 2011 2168 7522 1001 3114 1024 1055 1013 1052 19330 2102 1012 9345 2140 2005 7859 1010 2058 11066 17927 11616 1024 11290 19067 2966 4650 1024 3429 2095 2214 2158 4078 4017 18924 2015 1010 20014 19761 3064 3114 102\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   label: 1 (id = 1)\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   *** Example ***\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   guid: test-2\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   tokens: [CLS] cat ##het ##er projects over the right vent ##ric ##le below the out ##flow tract . no media ##sti ##nal widening or app ##re ##cia ##ble pl ##eur ##al e ##ff ##usion . no p ##ne ##um ##otho ##ra ##x . et tube is at the thor ##ac ##ic inlet . sin ##us rhythm with border ##line sin ##us ta ##chy ##card ##ia . generalized low voltage . delayed r wave progression with late pre ##cor ##dial q ##rs transition . findings are non - specific . clinical correlation is suggested . since the previous tracing of sin ##us ta ##chy ##card ##ia rate is slower and right pre ##cor ##dial lead t wave changes appear more prominent . tracing # 2 renal failure , acute [SEP]\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   input_ids: 101 4937 27065 2121 3934 2058 1996 2157 18834 7277 2571 2917 1996 2041 12314 12859 1012 2053 2865 16643 12032 17973 2030 10439 2890 7405 3468 20228 11236 2389 1041 4246 14499 1012 2053 1052 2638 2819 29288 2527 2595 1012 3802 7270 2003 2012 1996 15321 6305 2594 15824 1012 8254 2271 6348 2007 3675 4179 8254 2271 11937 11714 11522 2401 1012 18960 2659 10004 1012 8394 1054 4400 14967 2007 2397 3653 27108 27184 1053 2869 6653 1012 9556 2024 2512 1011 3563 1012 6612 16902 2003 4081 1012 2144 1996 3025 16907 1997 8254 2271 11937 11714 11522 2401 3446 2003 12430 1998 2157 3653 27108 27184 2599 1056 4400 3431 3711 2062 4069 1012 16907 1001 1016 25125 4945 1010 11325 102\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   label: 1 (id = 1)\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   *** Example ***\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   guid: test-3\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   tokens: [CLS] intake in products from or and here in sic ##u labs q 4 hr response : no response from fur ##ose ##mide labs showing cr taking bump from 0 to 8 plan : continue to monitor at ##n continue to monitor cr / bun / renal function respiratory failure , acute ( not ar ##ds / ) assessment : pt initially on ac 850 ##x ##14 , 40 % from or an ##esthesia settings ab ##gs getting more al ##kal ##otic post - fur ##ose ##mide doses o ##2 sat ##s ~ 95 % - 97 % briefly dropping o ##2 sat ##s ~ 90 and not rebound ##ing with su ##ction action : ab ##gs drawn q ##2 rt in to evaluate pt with drop [SEP]\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   input_ids: 101 13822 1999 3688 2013 2030 1998 2182 1999 14387 2226 13625 1053 1018 17850 3433 1024 2053 3433 2013 6519 9232 24284 13625 4760 13675 2635 16906 2013 1014 2000 1022 2933 1024 3613 2000 8080 2012 2078 3613 2000 8080 13675 1013 21122 1013 25125 3853 16464 4945 1010 11325 1006 2025 12098 5104 1013 1007 7667 1024 13866 3322 2006 9353 15678 2595 16932 1010 2871 1003 2013 2030 2019 25344 10906 11113 5620 2893 2062 2632 12902 20214 2695 1011 6519 9232 24284 21656 1051 2475 2938 2015 1066 5345 1003 1011 5989 1003 4780 7510 1051 2475 2938 2015 1066 3938 1998 2025 27755 2075 2007 10514 7542 2895 1024 11113 5620 4567 1053 2475 19387 1999 2000 16157 13866 2007 4530 102\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 23:15:52 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   label: 1 (id = 1)\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   *** Example ***\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   guid: test-4\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   tokens: [CLS] hc ##t > 30 ) trans ##fus ##e pt according to goals ? additional pr ##bc ##s to treat hc ##t ~ 26 - 27 for goal > 30 continue pathway sic ##u hp ##i : 45 ##yo m w / et ##oh ci ##rr ##hosis , mel ##d 28 , child class c ci ##rr ##hosis admitted for liver transplant . denies change in health since previous admission 8 / . admitted to for fever ##s , an ##emia , as ##cite ##s , ar ##f . hospital course , treated w / z ##os ##yn for c . per ##fr ##ingen ##s ba ##cter ##emia . para ##cent ##esis negative for sb ##p . e ##g ##d w / grade i var ##ices . [SEP]\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   input_ids: 101 16731 2102 1028 2382 1007 9099 25608 2063 13866 2429 2000 3289 1029 3176 10975 9818 2015 2000 7438 16731 2102 1066 2656 1011 2676 2005 3125 1028 2382 3613 12732 14387 2226 6522 2072 1024 3429 7677 1049 1059 1013 3802 11631 25022 12171 25229 1010 11463 2094 2654 1010 2775 2465 1039 25022 12171 25229 4914 2005 11290 22291 1012 23439 2689 1999 2740 2144 3025 9634 1022 1013 1012 4914 2000 2005 9016 2015 1010 2019 17577 1010 2004 17847 2015 1010 12098 2546 1012 2902 2607 1010 5845 1059 1013 1062 2891 6038 2005 1039 1012 2566 19699 15542 2015 8670 21162 17577 1012 11498 13013 19009 4997 2005 24829 2361 1012 1041 2290 2094 1059 1013 3694 1045 13075 23522 1012 102\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/14/2022 23:15:52 - INFO - __main__ -   label: 1 (id = 1)\n",
      "12/14/2022 23:16:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "12/14/2022 23:16:09 - INFO - __main__ -     Num examples = 4054\n",
      "12/14/2022 23:16:09 - INFO - __main__ -     Batch size = 2\n",
      "100%|███████████████████████████████████████| 2027/2027 [18:44<00:00,  1.80it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2022 23:34:54 - INFO - __main__ -   ***** Eval results *****\n",
      "12/14/2022 23:34:54 - INFO - __main__ -     RP80 = 0.014598540145985401\n",
      "12/14/2022 23:34:54 - INFO - __main__ -     eval_accuracy = 0.4605328071040947\n",
      "12/14/2022 23:34:54 - INFO - __main__ -     eval_loss = 0.7084383748115197\n",
      "12/14/2022 23:34:54 - INFO - __main__ -     global_step = 0\n",
      "12/14/2022 23:34:54 - INFO - __main__ -     training loss = 100000.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at Precision of 80 is {} 0.014598540145985401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = nn.Sigmoid()\n",
    "if do_eval:\n",
    "    eval_examples = processor.get_test_examples(data_dir)\n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    if local_rank == -1:\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "    else:\n",
    "        eval_sampler = DistributedSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    true_labels=[]\n",
    "    pred_labels=[]\n",
    "    logits_history=[]\n",
    "    for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss, temp_logits = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            logits = model(input_ids,segment_ids,input_mask)\n",
    "\n",
    "        logits = torch.squeeze(m(logits)).detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "\n",
    "        outputs = np.asarray([1 if i else 0 for i in (logits.flatten()>=0.5)])\n",
    "        tmp_eval_accuracy=np.sum(outputs == label_ids)\n",
    "\n",
    "        true_labels = true_labels + label_ids.flatten().tolist()\n",
    "        pred_labels = pred_labels + outputs.flatten().tolist()\n",
    "        logits_history = logits_history + logits.flatten().tolist()\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    df = pd.DataFrame({'logits':logits_history, 'pred_label': pred_labels, 'label':true_labels})\n",
    "\n",
    "    string = 'logits_clinicalbert_'+readmission_mode+'_chunks.csv'\n",
    "    df.to_csv(os.path.join(output_dir, string))\n",
    "\n",
    "    df_test = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
    "\n",
    "    fpr, tpr, df_out = vote_score(df_test, logits_history, readmission_mode, output_dir)\n",
    "\n",
    "    string = 'logits_clinicalbert_'+readmission_mode+'_readmissions.csv'\n",
    "    df_out.to_csv(os.path.join(output_dir,string))\n",
    "\n",
    "    rp80 = vote_pr_curve(df_test, logits_history, readmission_mode, output_dir)\n",
    "\n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,                 \n",
    "              'global_step': global_step_check,\n",
    "              'training loss': train_loss/number_training_steps,\n",
    "              'RP80': rp80}\n",
    "\n",
    "    output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download entire folder from AWS sagemaker to laptop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r -X ClinicalBERT3_results.zip './'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "472px",
    "left": "506px",
    "right": "20px",
    "top": "120px",
    "width": "742px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
